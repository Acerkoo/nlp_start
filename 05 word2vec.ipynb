{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.1\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from collections import Counter\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import d2l_pytorch as d2l\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 42068\n"
     ]
    }
   ],
   "source": [
    "with open('Data/data/ptb.train.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "print('# sentences: %d' % len(raw_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# token: 24 ['aer', 'banknote', 'berlitz', 'calloway', 'centrust']\n",
      "# token: 15 ['pierre', '<unk>', 'N', 'years', 'old']\n",
      "# token: 11 ['mr.', '<unk>', 'is', 'chairman', 'of']\n"
     ]
    }
   ],
   "source": [
    "for st in raw_dataset[:3]:\n",
    "    print('# token:', len(st), st[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "counter = Counter([tk for st in raw_dataset for tk in st])\n",
    "counter = dict(filter(lambda x:x[1] >= 5, counter.items()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# token: 887100\n"
     ]
    }
   ],
   "source": [
    "idx2token = [tk for tk, _ in counter.items()]\n",
    "token2idx = {tk:idx for idx, tk in enumerate(idx2token)}\n",
    "dataset = [[token2idx[tk] for tk in st if tk in token2idx] for st in raw_dataset]\n",
    "num_token = int(sum([int(len(st)) for st in dataset]))\n",
    "print('# token: %d' % num_token)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# token: 375592\n"
     ]
    }
   ],
   "source": [
    "def discard(idx):\n",
    "    return random.uniform(0, 1) < 1 - math.sqrt(\n",
    "        1e-4 / counter[idx2token[idx]] * num_token)\n",
    "\n",
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "print('# token: %d' % (sum([int(len(st)) for st in subsampled_dataset])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "'# the: before=50770, after=2198'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_counts(token):\n",
    "    return '# %s: before=%d, after=%d' % (token,\n",
    "                int(sum([st.count(token2idx[token]) for st in dataset])),\n",
    "                int(sum([st.count(token2idx[token]) for st in subsampled_dataset])))\n",
    "\n",
    "compare_counts('the')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'# join: before=45, after=45'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_counts('join')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size),\n",
    "                                 min(int(len(st)), center_i + window_size + 1)))\n",
    "            indices.remove(center_i)\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    print(contexts)\n",
    "    return centers, contexts"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9]]\n",
      "[[1], [0, 2], [0, 1, 3, 4], [2, 4], [2, 3, 5, 6], [4, 6], [4, 5], [8, 9], [7, 9], [8]]\n",
      "center 0 has contexts [1]\n",
      "center 1 has contexts [0, 2]\n",
      "center 2 has contexts [0, 1, 3, 4]\n",
      "center 3 has contexts [2, 4]\n",
      "center 4 has contexts [2, 3, 5, 6]\n",
      "center 5 has contexts [4, 6]\n",
      "center 6 has contexts [4, 5]\n",
      "center 7 has contexts [8, 9]\n",
      "center 8 has contexts [7, 9]\n",
      "center 9 has contexts [8]\n"
     ]
    }
   ],
   "source": [
    "tiny_dataset = [list(range(7)), list(range(7, 10))]\n",
    "print('dataset', tiny_dataset)\n",
    "# get_centers_and_contexts(tiny_dataset, 2)\n",
    "for center, context in zip(*get_centers_and_contexts(tiny_dataset, 2)):\n",
    "    print('center', center, 'has contexts', context)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_negatives(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while int(len(negatives)) < int(len(contexts)) * K:\n",
    "            if i == int(len(neg_candidates)):\n",
    "                i, neg_candidates = 0, random.choices(population, sampling_weights, k=int(1e5) )\n",
    "            neg, i = neg_candidates[i], i + 1\n",
    "\n",
    "            if neg not in set(contexts):\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "sampling_weights = [counter[w] ** 0.75 for w in idx2token]\n",
    "all_negatives = get_negatives(all_contexts, sampling_weights, 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert int(len(centers)) == int(len(contexts)) == int(len(negatives))\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (self.centers[item], self.contexts[item], self.negatives[item])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def batchify(data):\n",
    "    max_len = max(len(c)  + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = int(len(context)) + int(len(negative))\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * int(len(context)) + [0] * (max_len - int(len(context)))]\n",
    "    return (torch.tensor(centers).view(-1, 1), torch.tensor(contexts_negatives),\n",
    "            torch.tensor(masks), torch.tensor(labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "centers shape: torch.Size([512, 1])\n",
      "contexts_negatives shape: torch.Size([512, 60])\n",
      "masks shape: torch.Size([512, 60])\n",
      "labels shape: torch.Size([512, 60])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 4\n",
    "\n",
    "dataset = MyDataset(all_centers,\n",
    "                    all_contexts,\n",
    "                    all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                            collate_fn=batchify,\n",
    "                            num_workers=num_workers)\n",
    "\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(['centers', 'contexts_negatives', 'masks', 'labels'],\n",
    "                          batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "Parameter containing:\ntensor([[ 0.1019, -0.2441,  0.6801, -0.1640],\n        [ 0.5275, -0.3540, -0.9393,  1.6713],\n        [-0.8598,  1.2049,  0.1575,  0.8977],\n        [ 0.2519, -0.8882,  0.9017,  0.2163],\n        [ 0.7473,  1.3696,  1.2898,  0.3270],\n        [-2.2228, -0.8946,  1.7971, -1.2397],\n        [ 0.7047, -0.2511,  1.3355, -0.2577],\n        [ 1.9745,  0.3108,  0.0765, -0.2504],\n        [ 0.4052,  1.5951,  0.2573, -0.2861],\n        [-0.3335, -0.8295,  0.1491,  0.3676],\n        [-0.0200,  0.9105,  0.6291,  0.1826],\n        [ 2.0091,  0.8152,  0.8912,  0.8021],\n        [ 0.1772, -0.0799, -0.7612,  0.3121],\n        [ 0.1588, -0.7941,  0.5032,  0.7222],\n        [-1.0796,  0.0996,  1.3165,  1.7903],\n        [-1.0000, -0.5204,  0.3152,  0.1836],\n        [ 0.2163, -0.0470, -0.3259,  0.7394],\n        [ 1.5086, -2.0289, -1.1218, -0.2524],\n        [ 0.4902,  0.6436, -1.2170, -2.2503],\n        [-0.6141,  0.6577, -0.1521, -0.3868]], requires_grad=True)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "embed.weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0.5275, -0.3540, -0.9393,  1.6713],\n         [-0.8598,  1.2049,  0.1575,  0.8977],\n         [ 0.2519, -0.8882,  0.9017,  0.2163]],\n\n        [[ 0.7473,  1.3696,  1.2898,  0.3270],\n         [-2.2228, -0.8946,  1.7971, -1.2397],\n         [ 0.7047, -0.2511,  1.3355, -0.2577]]], grad_fn=<EmbeddingBackward>)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.long)\n",
    "embed(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1, 6])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((2, 1, 4))\n",
    "Y = torch.ones((2, 4, 6))\n",
    "torch.bmm(X, Y).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        '''\n",
    "        :param inputs: Tensor shape: (batch_size, len)\n",
    "        :param targets: Tensor of the same shape as input\n",
    "        :param mask:\n",
    "        :return:\n",
    "        '''\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1)\n",
    "loss = SigmoidBinaryCrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.8740, 1.2100])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.tensor([[1.5, 0.3, -1, 2], [1.1, -0.6, 2.2, 0.4]])\n",
    "\n",
    "label = torch.tensor([[1, 0, 0, 0], [1, 1, 0, 0]])\n",
    "mask = torch.tensor([[1, 1, 1, 1], [1, 1, 1, 0]])\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.float().sum(dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}